import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from src.vectorstore import get_vector_store
from src import CONFIG
from src.utils import format_docs, chat_stream

OPENAI_API_KEY = CONFIG["OPENAI_API_KEY"]
BASE_DIR = "./collections"

# --- VÃ©rification login ---
if "logged_in" not in st.session_state or not st.session_state.logged_in:
    st.error("ðŸš« AccÃ¨s refusÃ©. Veuillez vous connecter.")
    if st.button("ðŸ”‘ Retour Ã  la connexion"):
        st.switch_page("app.py")
    st.stop()

# Exemple page chat
if st.session_state.role == "admin":
    st.error("ðŸš« AccÃ¨s refusÃ©. Les admins ne peuvent pas accÃ©der au chat.")
    st.stop()

st.write(f"Bienvenue {st.session_state.nom} {st.session_state.prenom} ðŸ‘‹")

# --- Header avec bouton dÃ©connexion ---
col1, col2 = st.columns([8, 1])
with col1:
    st.title("ðŸ’¬ Interrogez vos documents")
with col2:
    if st.button("ðŸ”“ "):
        st.session_state.clear()  # on reset la session
        st.switch_page("app.py")

# --- Initialisation des conversations ---
if "conversations" not in st.session_state:
    st.session_state.conversations = {"Conversation 1": []}
if "active_conv" not in st.session_state:
    st.session_state.active_conv = "Conversation 1"

# --- Sidebar pour gÃ©rer les conversations ---
with st.sidebar:
    st.subheader("ðŸ’¬ Conversations")
    conv_names = list(st.session_state.conversations.keys())
    selected_conv = st.selectbox(
        "Choisir une conversation",
        conv_names,
        index=conv_names.index(st.session_state.active_conv)
    )
    st.session_state.active_conv = selected_conv

    if st.button("âž• Nouvelle conversation"):
        new_name = f"Conversation {len(conv_names) + 1}"
        st.session_state.conversations[new_name] = []
        st.session_state.active_conv = new_name
        st.rerun()

# --- PrÃ©paration du modÃ¨le et du retriever ---
vector_store = get_vector_store()

llm = ChatOpenAI(model="gpt-4o-mini", api_key=OPENAI_API_KEY, temperature=0.7)
#
# prompt = ChatPromptTemplate.from_template(
#     "RÃ©ponds Ã  la question suivante en utilisant le contexte.\n\n"
#     "Contexte:\n{context}\n\nQuestion:\n{question}\n\nRÃ©ponse:"
# )

template = """
Tu es Hugo, un assistant juridique spÃ©cialisÃ© dans le domaine bancaire. 
Ta mission est de rÃ©pondre uniquement aux questions juridiques relatives au secteur bancaire, en tâ€™appuyant exclusivement sur les documents fournis et les Ã©lÃ©ments de contexte.

RÃ¨gles de comportement :
- Si la question ne concerne pas le juridique ou le domaine bancaire, rÃ©ponds : Â« Merci de poser des questions juridiques dans le domaine bancaire ! Â».
- Si on te demande Â« Parle-moi de toi Â», Â« Qui es-tu ? Â» ou Â« Que sais-tu faire ? Â», tu peux rÃ©pondre briÃ¨vement Ã  propos de ton rÃ´le.
- Si la rÃ©ponse nâ€™est pas prÃ©sente dans les documents ou que tu nâ€™en es pas certain, dis simplement que tu ne sais pas. Nâ€™invente jamais de rÃ©ponse.
- Donne toujours une rÃ©ponse claire, concise (maximum trois phrases).
- Termine toujours ta rÃ©ponse par : Â« merci de m'avoir posÃ© la question ! Â».

Contexte : {context}

Question : {question}
"""

prompt =  PromptTemplate.from_template(template)

retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.5, "k": 5}
)

chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough(),
    }
    | prompt
    | llm
    | StrOutputParser()
)

# --- Affichage historique ---
for msg in st.session_state.conversations[st.session_state.active_conv]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- Input utilisateur ---
if user_input := st.chat_input("ðŸ’¬ Pose ta question ici..."):
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.conversations[st.session_state.active_conv].append(
        {"role": "user", "content": user_input}
    )

    response = chain.invoke(user_input)

    with st.spinner("GÃ©nÃ©ration de la rÃ©ponse..."):
        with st.chat_message("assistant"):
            stream_text = st.write_stream(chat_stream(response))

    st.session_state.conversations[st.session_state.active_conv].append(
        {"role": "assistant", "content": response}
    )
